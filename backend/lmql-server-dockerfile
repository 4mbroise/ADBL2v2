FROM nvidia/cuda:12.4.1-devel-ubuntu22.04
#FROM ubuntu:jammy

RUN apt-get update --yes --quiet && DEBIAN_FRONTEND=noninteractive apt-get install --yes --quiet --no-install-recommends \
    software-properties-common \
    build-essential apt-utils \
    python3.10 \
    pip \
    wget curl vim git ca-certificates kmod \
    nvidia-driver-525 \
 && rm -rf /var/lib/apt/lists/*

#RUN add-apt-repository --yes ppa:deadsnakes/ppa && apt-get update --yes --quiet
#RUN apt-get --yes install python3.10 pip

RUN git clone https://github.com/ggerganov/llama.cpp

# install llama.cpp and llama.cpp utilities
WORKDIR "/llama.cpp"
RUN make LLAMA_CUDA=1
RUN pip install sentencepiece --no-cache-dir
RUN CMAKE_ARGS="-DLLAMA_CUDA=on" pip install llama-cpp-python --no-cache-dir 

RUN pip install lmql[hf]
RUN pip install peft

WORKDIR /

COPY backend/src/downloadLocalModels.py /downloadLocalModels.py
COPY backend/src/models_config.json /models_config.json

RUN mkdir models
RUN python3 downloadLocalModels.py

COPY backend/entrypoint.sh /entrypoint.sh
ENTRYPOINT ["/entrypoint.sh"]

EXPOSE 4000

CMD ["lmql", "serve-model", "--port", "4000", "--cuda"]
